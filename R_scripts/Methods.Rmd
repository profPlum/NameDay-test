---
title: "PC True Naming Algorithm"
author: "Dwyer Deighan, Gokun Kul"
date: "9/26/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
source('./emb_funcs.R')

vocab_data = load_vocab_emb('../logs/com/vocab_emb_com.txt', '../logs/com/lexicon_com.csv', vocab_only=F, english_only=T)
vocab_emb = vocab_data$vocab_emb
PC_names = get_PC_names(vocab_emb)
colnames(vocab_emb) = rownames(PC_names)

interp_vocab_emb = vocab_emb %>% hard_max_emb_interp()
interp_PC_names = get_PC_names(interp_vocab_emb)
colnames(interp_vocab_emb) = rownames(interp_PC_names)

data = read_csv('../data/efw_cc.csv') %>% na.exclude()

# select only columns which are known members of more abstract columns
verbose_cols = grep('\\d[a-z]_', names(data))
verbose_data = data[,verbose_cols]
abstract_cols = grep('\\d_', names(data))
abstract_data = data[,abstract_cols]

# change names to a format that our algorithm can use
names(verbose_data) = gsub('\\d[a-z]?_', '', names(verbose_data))
names(verbose_data) = gsub('_', ' ', names(verbose_data))
```

<!-- TODO: add this to intro or background -->
## Discussion of Russian & Problem Paper
Two papers that heavily influenced this work were [1] and [2]. The first demonstrated that GloVE embeddings have little human interpretability at baseline (almost none), and that transforming the embedding space to define corresponding embedding dimension names could be done and was very useful for aligning with human intuition. However it relied heavily on a rare dataset called SEMCAT which was limited in scope, and therefore did not scale well. The other paper Rotations and Interpretability of Word Embeddings: the Case of the Russian Language, defined a very intuitive interpretability score $interp_k(V) = \sum V_{i,k}V_{j,k}(Vi Â·Vj)=(V^TVV^TV)_{k,k}$ (note that V=W in their paper) and demonstrated its properties under orthogonal transformations. Their paper was very useful for deriving insights about the problem. And while we didn't use many of their equations directly they inspired some aspects of ours. 

## True Naming Algorithm

### Setup & Assumptions:

For simplicity we refer to the word2vec model architecture. Word2vec demonstrates a couple of important concepts about word embeddings:
1. 1 hidden layer is sufficient to make them.
1. no activation function is necessary in that layer
1. no biases are necessary in that layer

This means that word2vec (and presumably word embeddings in general) can be represented as a simple linear transformation, we will call $E$, to get the vocab embeddings which we will refer to as $V \in R^{m,n} s.t. ||V_i||_2=1, \forall i, 1 \le i \le m$ where n := emedding size, m = vocab size. Given that inputs are encoded in a simple one-hot format that means that $EI = V^T => E=V^T$.

Now we are going to define two vectors spaces each of which we assume has a (unique?) semantic meaning at each point and each of which we assume to be fully expressive of all possible semantic meanings and their corresponding words. The first is the vocab space $S_v \subset R^m$ where each axis is a word but has many redundant dimensions. The second is the word embedding space $S_e \subset R^n$ which has little to no redundant dimensions.

Additionally we are going to assume that each embedding dimension has a specific semantic meaning regardless of whether that meaning has a corresponding word and that these meanings are best represented in $S_e$, we will define the corresponding embeddings as $P\in R^{n,n} s.t. P_i \in S_e$.

### Proof of True Naming algorithm:

Now observe that $V$ both fully expresses the coordinates of each $S_v$ axis in $S_e$ *and* the coordinates of each $S_e$ axis in $S_v$, along row and column dimensions respectively. Therefore we need only transform the coordinates of each $S_e$ axis in $S_v$ to the space $S_e$, i.e. $P=EV=V^TV$. This means that by definition when given a comprehensive set of vocab embeddings vectors we can directly derive the embeddings representing each embedding dimension for the corresponding vocab and embedding space.

## Correlation Property
It would also be intuitive and convenient if correlations of these embedding dimensions were reflected by the consine similarities of the derived dimension name embeddings $P$. Well it turns out on average that is true given some reasonable assumptions.

### Setup and Assumptions:

1. First we assume that $E(V_i)=0$, this is a mostly reasonable assumption since we have no reason to assume it would be biased positive or negatively. We tested this assumption using student's t-test and found it to be nearly accurate $CI(E(V_i), 0.05) = [0.004989122, 0.005431173]$ for the vocab we were using and GloVe-wiki-50 pretrained embeddings[CITEME].

1. We make the naive assumption that the expected value of the random variables $\theta$ and is the center of its extreme values that is $0 \le \theta_{x,y} \le m \implies E(\theta_{x,y})=m/2 $. Since $E(\theta) = \int_0^m\theta P(\theta)d\theta$, we are in other words assuming the distribution of $\theta$ is uniform.

Also note we use the notation $\theta_{x,y}$ to denote the *minimum* angle between the vectors x and y (i.e. $0 \le \theta \le \pi$).

### Proof:

The equation for cosine similarity is: $cos(\theta_{x,y}) = (x^T*y)/||x||_2||y||_2$ 

Lemma 1:
The equation for pearson's correlation coeficient is $r(x,y) = (\sum_{i=1}^n(x_i - \bar x)(y_i - \bar y))/\sqrt{\sum_{i=1}^n(x_i - \bar x)^2}\sqrt{\sum_{i=1}^n(y_i - \bar y)^2}$, given our assumption that $E(V_i)=0 \implies \bar x=0, \bar y=0 \implies r(x,y) = (\sum_{i=1}^n(x_i)(y_i))/\sqrt{\sum_{i=1}^n(x_i)^2}\sqrt{\sum_{i=1}^n(y_i)^2} = (x^T*y)/||x||_2||y||_2 = cos(\theta_{x,y})$

Lemma 2:
Lemma 1: $\implies r(V_{*,i}, V_{*,j}) = (V_{*,i}^T*V_{*,j})/||V_{*,j}||_2||V_{*,i}||_2$
$cos(\theta_{P_i,P_j}) = (P_i^T*P_j)/||P_i||_2||P_j||_2$
$P_i = P_{*,i} = V^TV_{*,i}$
$(P_i^T*P_j) = ((V^TV_{*,i})^T*V^TV_{*,j}) = ((V_{*,i}^TV)*V^TV_{*,j}) = (V_{*,j}^TV^T*VV_{*,i}) = V_{*,j}^TPV_{*,i}$ (i.e. new form for numerator)

Note that since $P=V^TV \implies \forall x: x^TPx \ge 0 \implies \theta_{x,Px} \le \pi/2$ (i.e. P is positive semi-definite)
Naive assumption: $0 \le \theta_{x,y} \le \pi/2 \implies E(\theta_{x,y})=\pi/4$

This is the bulk of the proof we've shown that the numerators are the same save a positive definite transformation. All that remains is consideration of offset  angle summation.

<!-- TODO: check this!! -->
Lemma 3:
$\theta_d = \theta_{V_{*,i}-V_{*,j},V_{*,i}-PV_{*,i}}$ (i.e. the angle between the two offset/difference vectors between $V_{*,j}$ and $V_{*,i}$, and $V_{*,i}$ and $PV_{*,i}$ respectively)

<!-- TODO: use cos() again as c() and check it is correct -->
$\exists c(\theta_d), -1\le c(\theta_d)\le1: \theta_{V_{*,j}, PV_{*,i}} = \theta_{V_{*,j}, V_{*,i}} + c(\theta_d)\theta_{V_{*,i},PV_{*,i}}$ (i.e. whether offset vectors are additive depends on angle between them)
Also naturally if the vectors are orthognal the the second one does not contribute to the total offset angle $\theta_{V_{*,j}, PV_{*,i}}$. Therefore $c(\pi/2) = 0$.

Naive assumption: $0\le\theta_d\le\pi \implies E(\theta_d) = \pi/2$
$E(\theta_d) = \pi/2 \implies E(\theta_{V_{*,i},PV_{*,i}}) = E(\theta_{V_{*,j}, V_{*,i}}) + E(\theta_{V_{*,i},PV_{*,i}})*E(c(\theta_d)) = E(\theta_{V_{*,j}, V_{*,i}}) = \pi/4$

Proof: 
Lemma 3 $\implies E(cos(\theta_{V_{*,i}, PV_{*,i}})) = cos(\pi/4) \approx 0.7071068 > 0 \implies E(V_{*,j}^TPV_{*,i}/||P_i||_2||P_j||_2) > 0$
Lemma 2 $\implies E(V_{*,j}^TPV_{*,i}/||P_i||_2||P_j||_2) = E((P_i^T*P_j)/||P_i||_2||P_j||_2) > 0$

<!-- TODO: justify this assumption with bayese theorem -->

<!-- TODO: finish this proof! -->

## Hard Max Interpretability Algorithm

![soft vs true names plot not found!]()

The problem with our algorithm at baseline is that although it can find the correct embeddings to represent existing embedding dimensions these embeddings don't necessarily (and in fact often don't in practice) correspond to real words. Then loss is introduced by translation back into natural language. Additionally the problem is compounded by the fact that since these embeddings don't correspond to words they also often don't correspond to word clusters and therefore example words that maximize this dimension are often borrowed from multiple nearby clusters.

So an ideal solution would be to find a transformation of the embeddings space $\tilde V=VH$ s.t. $\forall i: \tilde P_i \in \{\tilde V_i\}$, even if $H$ would introduce some error to word distances if it were low enough it would be tolerable for the sake of deriving interpretations.

Additionally it would be best if $\tilde P_i\tilde P_i^T \approx I$, this would imply as little overlap between concepts on each dimension as possible. So we should choose our target names to be as orthogonal as possible.

### Greedy Orthogonal name selection:
We decided to use a simple greedy algorithm to choose target dimension name embeddings $G$. The algorithm is as follows:

$G_0 = V_i: i = argmax_i(||V_i||_\infty)$
for (i=2..m) {
$L_i = ||V_iG^T||_2: i=1..m$
$m = argmin_i(L_i)$
$G_i = V_m$
}

What we are doing is setting the first $G$ to the word embedding with the largest value in any dimension, assuming that when vectors most orthogonal to this are chosen they will lie nearly on top of an axis. We then proceed to check each candidate vector's similarity to each existing row of G and choose to minimize the L2 norm of this vector. That is because the L2 norm will prioritize minimization of the maximum similarity among existing vectors, but will also account for similarity to other vectors as well some degree. Then whichever candidate has the lowest loss L is chosen and added to G, and then the process continues until no more names are needed.

### Method for Name Setting Proof:
Given our greedily chosen target name embeddings $G$ we need $(VH)^T$
$\tilde V = VH$

$GH = \tilde P = \tilde V^T\tilde V=(VH)^T(VH) =H^TV^T(VH)$
$(H^T)^{-1}GHH^{-1} = V^TV$
$(H^T)^{-1}G = V^TV$
$(H^T)^{-1} = V^TVG^{-1}$
$H = ((V^TVG^{-1})^{-1})^T$

## Discussion:
In this paper we demonstrate that word embedding dimensions do indeed have names which are already inherent in their structure. And that by reassigning those names to more useful ones. Notably roughly orthogonal names which correspond to real words. It can be extremely useful for interpreting the embedding space and for naming embeddings of other types of data given that the source data has named features. In particular this is quite useful for data summary visualizations which naturally benefit from low dimensionality and named dimensions. We also demonstrated that these dimension name embeddings benefit from intuitive properties like correlation between dimension embedding similarity and dimension correlation on average. We also were able to create a generalized algorithm to do this that worked purely on existing word embeddings of any kind. This makes our algorithm extremely scalable and versatile applying to a wide range of data visualization tasks as well as potentially semi-automated data simplification in a database setting. Lastly our algorithm could even be extended to automatic naming of hidden neurons in any neural network with all named input features. The total possible scope of the algorithm is enormous and is backed by formal proofs and empirical data.

## Data Visualization Demo:

### Dataset:
Here we test our algorithm on a real data set to demonstrate its utility in a practical setting. Luckily we were able to find a dataset that is unique in that it actually has ideal PCs and names built in. The data is the economic world freedom dataset found on kaggle[3]. The data structure is such that there are 5 macro variables and as many constituent variables for each macro variable (from which macro variables are directly computed). Since we already know what a human expert has determined to be good summary variables we can compare them directly to the results of the algorithm.

### Methods:
* We first scaled the dataframe so that PCA could work properly.
* We defined the column embeddings as the rotation matrix of PCA.
* Column embeddings were essentially given the same treatment as vocab embeddings were previously, each embedding vector was L2 normalized and then transformed using hard max interpretability. We found that using the hard max interpretability algorithm we invented on the column embeddings helped in interpretability of the resulting variables.
* We also used hard max interpretability on the vocab embeddings naturally. Using it on both helped the most.
* To get column name embeddings we tokenized each column name into words then formed a dtm from column names where each name was considered a document. Then we took dtm *dot* vocab_emb to transform into the word embedding space.
* To get names of the new interpretable PCs we used the true naming algorithm with some modifications. This time we passed in the name embeddings to be transformed by the column embeddings. Then we took the similarity to each vocab embedding to find the closest word. Again we used hard max interpretability on the vocab embeddings.

That is:
$t(x) :=$ *tokenize* x: return set of words found in string x 
$\breve H(x) :=$ *hard* max interpretability on x
$\grave N, \grave V:=$ vector of column *names*, and *vocab* words respectively
$N:=$ column *name* embeddings
$D_{i,j} = \begin{cases} 1 &\mbox{if } \grave V_j \in t(\grave N_i) \\ 0 & \mbox{otherwise }  \end{cases} :=$ *document* term matrix of column names
$C_i = C_i/||C_i||_2$
$\tilde C = \breve H(C)$
$\grave N = D\tilde V$
$P = \tilde C^TN$

### Justification:
  Again we are maximizing the orthogonality of the embeddings of the embedding dimensions, this time of the columns of a dataframe. This minimizes the overlap between concepts reresented by each column embedding dimension, which of course is good if we would like to name them. Since the embeddings of the column data is capable of representing individual existing columns and the embedding dimensions themselves are replacement columns it is capable of representing them as well just as in the word embedding case.

<!-- TODO: justify this better -->
We normalized the column embedding vectors simply because we wanted greater consistency with how we handled vocab embeddings and normalization is generally beneficial in a wide variety of contexts. 

  With regards to the creation of $P = \tilde C^TN$ we can no longer treat the word embeddings ($N$ in this case) as the weight matrix with which to encode itself. Now instead real data is being encoded and we must simply encode the word embeddings which correspond to that data in a like manner. Indeed in addition to being intuitive this method gave us the best results.

```{r, echo=T}
short_hand = list(marg='marginal', gov='government', std='standard', ppl='people', reg='regulation')

simplify_df = function(vocab_emb, df, rank, term_mat=.term_mat_choices) {
  vocab = rownames(vocab_emb)
  PCA = prcomp(df, rank=rank, scale=T)
  
  # df = t(one_hots), PCA$rotation = t(weights)
  # that is why there is left multiplication
  stopifnot(all.equal(scale(df) %*% PCA$rotation, PCA$x))
  
  # we can treat this just like a word embedding problem
  # the reason that 
  col_emb = norm_emb(PCA$rotation)
  interp_col_emb = col_emb %>% hard_max_emb_interp()
  #compare_interp_scores(interp_col_emb)
  
  heatmap(interp_col_emb)
  
  # make DTM from column names
  # assumes no repeat words in colnames!
  colname_dtm = matrix(0, ncol(df), nrow(vocab_emb))
  dtm_mask = colnames(df) %>% strsplit(' ') %>% map(~if_else(.x %in% names(short_hand), short_hand[.x], as.list(.x))) %>%
    map(as_vector) %>% walk(~stopifnot(all(.x %in% vocab))) %>% map(~vocab %in% .x) %>% reduce(rbind)
  colname_dtm[dtm_mask] = 1
   
  # same format as vocab embeddings
  colname_emb = colname_dtm %*% vocab_emb
  
  # here colname_emb takes place of vocab_emb
  # and t(PCA$rotation)=W
  PC_names = get_PC_names(colname_emb, vocab_emb = vocab_emb,
                          emb_weights=t(interp_col_emb))
  
  colnames(interp_col_emb) = rownames(PC_names)
  View(get_maximal_examples(interp_col_emb))
  
  # VERIFIED TO WORK
  simple_df = scale(df) %*% interp_col_emb
  colnames(simple_df) = rownames(PC_names)
  return(as_tibble(simple_df))
}

(simple_df = simplify_df(interp_vocab_emb, verbose_data, rank=5))

# TODO: make this more expressive
main_PCs = simple_df[,1:2]
plot(main_PCs[[1]], main_PCs[[2]], main='simple PC plot',
     xlab=names(main_PCs)[[1]], ylab=names(main_PCs)[[2]])

cor(simple_df, abstract_data)
```

## Interpretability Scores:

Note that for the standard metric we could not compute it exactly because we were using pretrained embeddings and therefore didn't have the corresponding term-cooccurrence matrix. So instead we approximated this using a matrix of cosine similarities between the relevant maximal vocab embeddings for each dimension.

We use two different interpretability scores to evaluate our hard max interpretability algorithm. The scores measure the interpretability of the dimensions themselves and not the names assigned to them. One interpretability score is the one proposed by [1] and the other is the one that is used most often in the related literature[1][4]. For the standard metric most widely used, there is a significant increase in the mean and median of the score.

### Baseline GloVe Embeddings:
```{r}
compare_interp_scores(vocab_emb)
image.real(t(PC_names)%*%PC_names, 'P^TP')
```

### Hard Max Interpretability Embeddings:
```{r}
compare_interp_scores(interp_vocab_emb)
image.real(t(interp_PC_names)%*%interp_PC_names, 'P^TP')
```

For the metric proposed in [1] our algorithm got a median that was higher and the plots demonstrate that our distribution of interpretability is desirable compared to the baseline. However we *did get a lower mean* interpretability score, surprisingly. That being said there could be a reasonable explanation for this below.

Also note the marked difference in the $P^TP$ matrix images. It can clearly be seen that after our hard max algorithm $P$ (or rather $\tilde P$) becomes much closer to orthogonal given how closely $\tilde P^T\tilde P$ resembles the identity matrix.

#### Important Consideration:

  Recall the Zobnin interpretability score: $interp_k(V) = \sum V_{i,k}V_{j,k}(Vi Â·Vj)=(V^TVV^TV)_{k,k}$. Interesting and intuitively these equations state that total interpretability $= tr(V^TVV^TV)$. This implies that orthogonality of embedding dimension names $P=V^TV$ is a good thing since under a size constrain such as $P_i = P_i/||P_i||_2$ (i.e. requiring normalization of embedding vectors as is the case in $V$), orthogonal names is how you would maximize this quantity. 
  This makes a good argument for the fact that it should be required that $P_i = P_i/||P_i||_2$ and that total Zobnin interpretability $= tr(PP)$. And in fact when this is the case, we not only beat baseline scores (mean: 0.9728276 -> 0.9932468,  and median: 0.9596717 -> 0.9952927). but also observe that the scores appear to now have the desirable property $interp_k(V) \le 1$ (versus unbounded previously).

Despite this we do not currently require that $P_i = P_i/||P_i||_2$, in our work. That being said we tested how doing so would affect decoded embedding names and we didn't observe any changes.

### Maximal Dimension Examples:

```{r maximal examples}
get_maximal_examples(interp_vocab_emb)
```

Here each column represents an embedding dimension and the rows are the top 5 words which have the highest values in that dimension (sorted in descending order). The name of each column is the name assigned to the embedding dimension by our algorithm. As you can see there is a clear coherent structure to each embedding dimension and the names assigned to the dimensions are sometimes so similar to the maximal examples that they themselves appear in the maximal examples.

## Considering Word Embedding Integrity Post Transformation:

```{r}
cat('mean absolute cos sim difference:', cos_sim_diff(vocab_emb, interp_vocab_emb), '\n')
```

Here we explore to degree to which the word embeddings similarities to one another have be perturbed by the hard max interpretability algorithm. The two scatter plots 'before' and 'after' H transform give a crude approximation of word embedding's points in hyperspace before and after. We created these plots using principle coordinate analysis which is analogous to PCA but directly attempts to preserve hyperspacial point distances in the lower dimensional representation (where in this case distance $=-cos(\theta_{x,y})$). It should be noted since this is a crude approxmiation you should mostly be attentive to the distances between words rather than their points in space, none the less it is an example of how word similarites might change under this transformation. We kept the sample size low to avoid plot bloat.

As can be seen in the distributional plots, in the extreme cases some word similarities have been affected by up to around 0.5 (excluding outliers). However the vast majority have an absolute difference $\le 0.2$, and indeed the mean is only 0.142986. So it although it should be noted that this algorithm can't make hard guarentees to the resulting quality of word embeddings, it shouldn't matter too much since for the large majority of words their embeddings are only slightly affected. 
<!-- Especially since in an EDA context fast understanding is far more important than perfection. -->
Also empiracle evaluation of the word embeddings post transformation are very promising, suggesting that word similarities remain mostly intact to human intuition.

## Works Cited:
1. Interpretability of Word Embeddings: the Case of the Russian Language[CITEME]
1. Semantic Structure and Interpretability of Word Embeddings[CITEME]
1. Economic World Freedom Dataset[CITEME]
1. Reference to common use of std interp score[CITEME]

